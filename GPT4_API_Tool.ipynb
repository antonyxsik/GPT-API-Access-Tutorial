{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4 API Access Workflow \n",
    "\n",
    "I primarily use this tool to gain access to the GPT4 Turbo API for the long context window. Essentially all I do is read in a long text file, give it to GP4 via the API, and in return I receive a text file response. The following describes my setup and process of how to use this tool to get information from academic papers. \n",
    "\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Download the source file of the paper from arxiv (usually as a tar.gz), and unzip it to find the .tex source file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Convert the .tex file into a .md file using pandoc. This makes it easier for GPT4 to process (less formatting and weird characters). After installing pandoc (https://pandoc.org/installing.html), run a command such as ``` pandoc -s mathpaper_source.tex -o mathpaper_source.md``` in terminal (in the correct directory) to convert. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Create a text file, in which you write your prompt and paste in any of the contents you want from your paper. An example can be found at \"sample_prompt.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Sign up for the OpenAI API. I forget whether you have to pay or request access or how this works (I did it long ago), but I remember that each time you connect to these models it costs money. Luckily, I think it costs roughly 1 cent for 1000 tokens or even less. Unsure. I put $20 up there and have used less than $1. You can count the number of tokens in your text file using https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Get your API key. Should be somewhere in your profile on the OpenAI website. Save the key in a blank, unnamed .env file where you write \"OPENAI_API_KEY=paste_your_key_here\". An example can be found at \".env\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Run the following code. You should receive a new text file in your directory which contains the GPT4 response. It should also print below. An example can be found at \"wildfire_api_response.txt\"\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing necessary packages, importing libraries, connecting to API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install openai \n",
    "#%pip install python-dotenv\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "#can print to check and make sure it's correct\n",
    "#print(openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the function to query GPT4 below. \"gpt-4-1106-preview\" is the tag for the turbo model with the long context window. Other tags (and their token per minute limits) can be found here: https://platform.openai.com/account/limits . \n",
    "\n",
    "Please notice the ```temperature``` setting below. This should be between 0 and 1, and loosely dictates the \"randomness\" of the model. A temperature of 0 means the same answer every time. Temperatures of 0.7-0.9 are generally recommended for creative tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently speaking with gpt4 turbo\n",
    "def get_completion(prompt, model=\"gpt-4-1106-preview\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.8, # degree of randomness for model output \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in text file which will be used as prompt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace wildfire with your file\n",
    "with open('sample_prompt.txt', 'r', encoding='utf-8') as file:  \n",
    "    file_contents = file.read()\n",
    "\n",
    "#string prompt\n",
    "gpt4_prompt = file_contents\n",
    "\n",
    "#inaccurate but close token counter\n",
    "tokens_nltk = word_tokenize(gpt4_prompt)\n",
    "print(len(tokens_nltk))\n",
    "\n",
    "#printing for sanity\n",
    "print(gpt4_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the response: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_response = get_completion(gpt4_prompt)\n",
    "\n",
    "#printing for sanity: \n",
    "print(gpt4_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the response to a txt file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file path where you want to save your string\n",
    "file_path = 'sample_response.txt'  \n",
    "\n",
    "# Open the file in write mode and save the string to it\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(gpt4_response)\n",
    "\n",
    "print(f'File saved as {file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
